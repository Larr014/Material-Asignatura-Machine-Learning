{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cBWzrPfQzOOg",
        "outputId": "380098bf-bafe-4dd6-e045-5318c4eabef3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matriz de Confusión:\n",
            " [[4 1]\n",
            " [1 4]]\n",
            "Exactitud: 0.8\n",
            "Precisión: 0.8\n",
            "Sensibilidad: 0.8\n",
            "F1 Score: 0.8000000000000002\n",
            "AUC-ROC: 0.8\n",
            "MAE: 0.475\n",
            "MSE: 0.2874999999999999\n",
            "RMSE: 0.5361902647381803\n",
            "R²: 0.9605995717344754\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# Datos de ejemplo\n",
        "y_true = [1, 0, 1, 1, 0, 1, 0, 0, 1, 0]\n",
        "y_pred = [1, 0, 1, 0, 0, 1, 0, 1, 1, 0]\n",
        "\n",
        "# Métricas de clasificación\n",
        "conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "precision = precision_score(y_true, y_pred)\n",
        "recall = recall_score(y_true, y_pred)\n",
        "f1 = f1_score(y_true, y_pred)\n",
        "roc_auc = roc_auc_score(y_true, y_pred)\n",
        "\n",
        "print(\"Matriz de Confusión:\\n\", conf_matrix)\n",
        "print(\"Exactitud:\", accuracy)\n",
        "print(\"Precisión:\", precision)\n",
        "print(\"Sensibilidad:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "print(\"AUC-ROC:\", roc_auc)\n",
        "\n",
        "# Métricas de regresión (usando datos de ejemplo)\n",
        "y_true_reg = [3.0, -0.5, 2.0, 7.0]\n",
        "y_pred_reg = [2.5, 0.0, 2.1, 7.8]\n",
        "\n",
        "mae = mean_absolute_error(y_true_reg, y_pred_reg)\n",
        "mse = mean_squared_error(y_true_reg, y_pred_reg)\n",
        "rmse = mean_squared_error(y_true_reg, y_pred_reg, squared=False)\n",
        "r2 = r2_score(y_true_reg, y_pred_reg)\n",
        "\n",
        "print(\"MAE:\", mae)\n",
        "print(\"MSE:\", mse)\n",
        "print(\"RMSE:\", rmse)\n",
        "print(\"R²:\", r2)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import label_binarize\n",
        "# Datos de ejemplo con tres categorias\n",
        "y_true = [0, 1, 2, 0, 1, 2, 0, 1, 2, 0]\n",
        "y_pred = [0, 2, 1, 0, 1, 0, 0, 2, 2, 0]\n",
        "\n",
        "# Métricas de clasificación\n",
        "conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "\n",
        "# Calculamos precision, recall y f1 para cada clase\n",
        "precision = precision_score(y_true, y_pred, average=None)\n",
        "recall = recall_score(y_true, y_pred, average=None)\n",
        "f1 = f1_score(y_true, y_pred, average=None)\n",
        "\n",
        "print(\"Matriz de Confusión:\\n\", conf_matrix)\n",
        "print(\"Exactitud:\", accuracy)\n",
        "print(\"Precisión por clase:\", precision)\n",
        "print(\"Sensibilidad por clase:\", recall)\n",
        "print(\"F1 Score por clase:\", f1)\n",
        "y_true_binarized = label_binarize(y_true, classes=[0, 1, 2])\n",
        "y_pred_binarized = label_binarize(y_pred, classes=[0, 1, 2])\n",
        "roc_auc = roc_auc_score(y_true_binarized, y_pred_binarized, average=None)\n",
        "\n",
        "print(\"AUC-ROC por clase:\", roc_auc)\n",
        "\n",
        "# Métricas de regresión (usando datos de ejemplo)\n",
        "y_true_reg = [3.0, -0.5, 2.0, 7.0]\n",
        "y_pred_reg = [2.5, 0.0, 2.1, 7.8]\n",
        "\n",
        "mae = mean_absolute_error(y_true_reg, y_pred_reg)\n",
        "mse = mean_squared_error(y_true_reg, y_pred_reg)\n",
        "rmse = mean_squared_error(y_true_reg, y_pred_reg, squared=False)\n",
        "r2 = r2_score(y_true_reg, y_pred_reg)\n",
        "\n",
        "print(\"MAE:\", mae)\n",
        "print(\"MSE:\", mse)\n",
        "print(\"RMSE:\", rmse)\n",
        "print(\"R²:\", r2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t99RR-3E2a-w",
        "outputId": "1e2d8634-9d48-42d1-c148-9c1c81e388a2"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matriz de Confusión:\n",
            " [[4 0 0]\n",
            " [0 1 2]\n",
            " [1 1 1]]\n",
            "Exactitud: 0.6\n",
            "Precisión por clase: [0.8        0.5        0.33333333]\n",
            "Sensibilidad por clase: [1.         0.33333333 0.33333333]\n",
            "F1 Score por clase: [0.88888889 0.4        0.33333333]\n",
            "AUC-ROC por clase: [0.91666667 0.5952381  0.52380952]\n",
            "MAE: 0.475\n",
            "MSE: 0.2874999999999999\n",
            "RMSE: 0.5361902647381803\n",
            "R²: 0.9605995717344754\n"
          ]
        }
      ]
    }
  ]
}